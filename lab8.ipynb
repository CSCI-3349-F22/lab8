{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 8: Transformer language models for classification\n",
        "\n",
        "This lab was adapted from a few different Huggingface tutorials and notebook including [this](https://huggingface.co/blog/sentiment-analysis-python) and [this](https://huggingface.co/docs/transformers/training#finetune-with-trainer). \n",
        "\n",
        "This lab demonstrates how to use [DistilBERT](https://arxiv.org/abs/1910.01108), a light-weight version of BERT that uses many fewer resources while being nearly as accurate on important benchmarks. If you would like to use DistilBERT for your projects, [this Hugginface page](https://huggingface.co/docs/transformers/model_doc/distilbert) has links to many different notebooks for various kinds of tasks. *Pro tip: cribbing from other people's Colab notebooks is basically how this kind of work is done. Just make sure to acknowledge your sources.*\n",
        "\n",
        "We will work with two datasets. The first is a freely available set of 50K movie reviews from IMDB, which is available through the Huggingface datasets library. The second is the good old clickbait vs. real news headline database. You will adapt the code I've given you for the IMDB dataset to the clickbait data to show me that you know how to convert your own datasets into the expected format for DistilBERT for classification.\n",
        "\n",
        "For this lab, you will turn in **this notebook with all the output** so that we can see that you ran everything and with the (very few) questions answered in the appropriate places. This lab is due on **Thursday, November 10, 2022, at 11:59pm EST**."
      ],
      "metadata": {
        "id": "NxrlNCXYpFx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setting things up\n",
        "### Activate the GPU. \n",
        "Using the GPU will make everything much faster, and the free verison of Colab allows you to use a GPU for limited periods of time.  To activate a GPU, go to the `Runtime` menu, then select `Change runtime type`, then pick `GPU` from the dropdown menu.\n",
        "\n",
        "If you want to remain on good terms with Colab, just remember to disconnect from the GPU when you are done with it (`Runtime-> Disconnect and delete runtime`). Colab will disconnect you automatically after idle time anyway, but it's wise to try to use as few resources as possible."
      ],
      "metadata": {
        "id": "0RHGuKJxnoav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount your Google Drive\n",
        "You're going to be be loading the clickbait dataset from your drive later on, so let's mount it now. \n",
        "\n",
        "**Don't forget to create a folder in your Drive called `lab8` where you can put the files I included in the repo for the clickbait dataset.**"
      ],
      "metadata": {
        "id": "rgXiqk0YwGLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "y9oGyHU1o80r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install some libraries\n",
        "Next, let's install some libraries. The first, [datasets](https://huggingface.co/docs/datasets/index), is Huggingface library that lets programmers easily download and process NLP datasets. The second is the [transformers](https://huggingface.co/docs/transformers/index) library, which is one of the most widely-used libraries for downloading and training pre-trained models for NLP."
      ],
      "metadata": {
        "id": "x-8ln-GXpKrz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uxV80C-5d5n"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Loading and processing the data\n",
        "Now we will load the IMDB dataset from datasets. It contains 50K movie reviews that are very positive or negative, with half for training and half for testing. It will take a bit of time to download, but conveniently, you get a nice progress bar so you'll have some idea of how long you'll need to wait. **Don't go away! You'll want to sit by your computer until you start training, a few steps from now.**"
      ],
      "metadata": {
        "id": "DlOQ0D7BpSSG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuTAwY7K4v_3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For fun later on, you can train with the whole dataset, but one of the things that is cool about BERT and friends is that they are pre-trained on billions of words, so you don't need that much data to fine-tune to your task. Let's use a **subset of 3000 from the training set and 300 from the test set**. The code below selects a random subset of 3000 and 300."
      ],
      "metadata": {
        "id": "Nd6T6T7Gpmzt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwJyWS_544sd"
      },
      "outputs": [],
      "source": [
        "small_train_dataset = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(3000))])\n",
        "small_test_dataset = imdb[\"test\"].shuffle(seed=42).select([i for i in list(range(300))])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print out a few training examples to see what they look like. After the text, you can see the labels, which are 1=positive and 0=negative."
      ],
      "metadata": {
        "id": "x0INXF_r78Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_dataset[0]\n",
        "small_train_dataset[2]"
      ],
      "metadata": {
        "id": "bqHiehO3772O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You remember **tokenization** from the very beginning of the year, and in fact, it's still incredibly important! Even with all the fancy neural networks, it's still necessary to tokenize your text. The AutoTokenizer below will tokenize your data so that it plays nicely with the `distilbert-base-uncased` model."
      ],
      "metadata": {
        "id": "ifLL4WfeqCc-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjcm-1I247Lz"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a little function that will run the tokenizer for you. You pass in one of your datasets (in this case, either the training or the test set), and then it will tokenize whatever is in the \"text\" field of each row of the dataset. The `truncation=True` part just truncates texts that are longer than some max length."
      ],
      "metadata": {
        "id": "oIG-A2LRqQ0t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi9RAF8J4_Rt"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "  return tokenizer(examples[\"text\"], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll run the tokenizer function on our train and test sets, and we'll save the results out to new variables."
      ],
      "metadata": {
        "id": "jFcq8wCyxqeo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp_Fvhz4566O"
      },
      "outputs": [],
      "source": [
        "# (removing batched=True)\n",
        "tokenized_small_train = small_train_dataset.map(preprocess_function)\n",
        "tokenized_small_test = small_test_dataset.map(preprocess_function)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do they look like now? You'll see that the tokenization step is more than just separating punctuation and that sort of thing! Each object still has the original text and the label, but now there are two additional components: a list of input IDs (i.e., unique integer IDs for each token in the intput sequence) and a list of the same length full of 1s, called the \"attention mask\". This will be used later on during padding. Tokens that are non-padding tokens will have an attention mask of 1, while tokens that are padding tokens will have an attention mask of 0. (Padding is discussed a bit below.)"
      ],
      "metadata": {
        "id": "IQFu0izo8e5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_small_train[0]"
      ],
      "metadata": {
        "id": "WoQ_WkTF8fIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we are getting ourselves a **data collator** that will later be used to convert the training samples to PyTorch tensors and to make sure they have the correct amount of padding. Training typically expects every input sample to be the same length. You can ensure this by \"padding\" the inputs shorter than the maximum length with (usually) a lot of trailing zeros at the beginning or the end or both."
      ],
      "metadata": {
        "id": "Y98UNk4-q8pz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yn-jSp87Z35"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Setting up the training (a.k.a. the fine-tuning)\n",
        "Now we're going to do a few things to get everything ready for actually doing the fine-tuning of  DistilBERT to our task of classifying movie reviews according to their sentiment.\n",
        "\n",
        "First, we need to download the model for classification, which is the `disilbert-based-uncased` model. Conveniently, Huggingface has set the whole thing up for us to do classification easily. We don't actually need to write the softmax layer or anything like this since they have done it all for us. We just need to get the pretrained model designed specifically for sequence classification.\n",
        "\n",
        "You will get a warning saying that the weights were not used when intializing. That makes sense since we are fine-tuning to a new task, so it's okay."
      ],
      "metadata": {
        "id": "54CJf0sm2_Wk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu0IJJ-X7u77"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we are done with our training, we want to be able report the metrics that are actually interesting to us, not just loss, which is what is typically reported. So let's define what we want to see: accuracy and f1."
      ],
      "metadata": {
        "id": "i5-vaxp7rpno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        " \n",
        "def compute_metrics(eval_pred):\n",
        "   load_accuracy = load_metric(\"accuracy\")\n",
        "   load_f1 = load_metric(\"f1\")\n",
        "  \n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "   return {\"accuracy\": accuracy, \"f1\": f1}"
      ],
      "metadata": {
        "id": "PK8nPTr3rym-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are setting all the hyperparameters and arguments we need for training. The `TrainingArguments` object contains many the hypterparameters we've talked about for training neural nets, like batch size, learning rate, number of epochs, and weight decay.\n",
        "\n",
        "The `Trainer` object takes the training arguments as one of its arguments, along with the model (DistilBERT), the data collator that converts training instances into tensors of the right length, the set of evaluation metrics we want to use, and pointers to the training and testing data."
      ],
      "metadata": {
        "id": "9WFmytXw4HOO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CKCUN0A78Zl"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/lab8/results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_small_train,\n",
        "    eval_dataset=tokenized_small_test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Training\n",
        "\n",
        "The code cell below will do the training, i.e., the fine-tuning of DistilBERT to the task of sentiment classification, on this small dataset of movie reviews.  When I ran this with a GPU, it took 12 minutes or so."
      ],
      "metadata": {
        "id": "4-XVOdNk5DO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "zNDcnvFFmvSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Evaluation\n",
        "\n",
        "The code cell below will evaluate the fine-tuned model with the test data. How does it know what data to use? We gave it a pointer to the test data when we created the Trainer objects a few cells ago. This step should be quick."
      ],
      "metadata": {
        "id": "whqXqWTYsRVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx2nHiTe79Br"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I got F1=88.2 and accuracy=88. You'll probably get something similar but it won't be exactly the same since the dataset was shuffled early on. Not bad accuracy at all, though I think this dataset is meant to be easy. Now we'll try with a different dataset, where you will be writing (copying and pasting) the code."
      ],
      "metadata": {
        "id": "aVkYhDlz7tsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Doing it all over again with our own data\n",
        "\n",
        "Now I would like you to write (copy and paste) the code from above to fine-tune and test on a different dataset, namely, the clickbait vs. new headlines dataset. In the repo, I distributed new versions of the data in a format that will be compatible with `load_dataset()`. Don't forget to put those two files in a folder on your Google Drive called `lab8`.\n",
        "\n",
        "The remainder of this notebook will mostly involve (1) being extremely fastidious about replacing variable names, and (2) figuring out which steps you need to do and which ones you don't. \n",
        "\n",
        "I am getting you started below since the process of loading a dataset from a csv is different from downloading a dataset from Huggingface. \n"
      ],
      "metadata": {
        "id": "NP09u6Gi-pxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the csv files I provided into a dataset\n",
        "clickbait = load_dataset('csv',data_files={'train': '/content/drive/MyDrive/lab8/click_train.txt', \n",
        "                                           'test': '/content/drive/MyDrive/lab8/click_test.txt'})\n",
        "\n"
      ],
      "metadata": {
        "id": "F2RqgnirBFXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Insert the rest of your code in this code block and other additional code blocks.\n",
        "## Don't waste time installing libraries or setting variables that have already been set."
      ],
      "metadata": {
        "id": "kzcvyJbSJgL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q1 What accuracy and F1 did you get on the clickbait dataset? How does this compare with the various approaches you used previously for this dataset?"
      ],
      "metadata": {
        "id": "RKZ8yEE5scwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enter your response to Q1 here."
      ],
      "metadata": {
        "id": "E1DrUXYvspLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2 Do you think you will use a BERT or related approach in your project? Why or why not?"
      ],
      "metadata": {
        "id": "uPV4GOdFstDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enter your response to Q2 here."
      ],
      "metadata": {
        "id": "fv1JlCxOs0bE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}